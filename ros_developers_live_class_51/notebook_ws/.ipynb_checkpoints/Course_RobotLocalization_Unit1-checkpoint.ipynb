{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Robot Localization package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Robot-localization-cover.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robotignite_logo_text.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: Merging sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time to completion: **2 hours**\n",
    "\n",
    "In this Unit you are going to learn how to use the **robot_localization** package to merge data from different sensors in order to improve the pose estimation for localizing your robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">END OF SUMMARY</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with robots in real life scenarios, sometimes we have to deal with sensor data which is not very accurate. And if we want to perform a good localization for our robot, we need to have the most accurate sensor data we possibly can. For this purpose, one of the solutions is to merge data from different sensors. The more data we have, the better the robot will be able to sense the world around him. And here it is when the **robot_localization** package comes in handy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many advantages that the **robot_localization** package provides, is the ability to merge data from many sensors. Actually, you could merge data from infinite sensors, if you are patient enough to place them on your robot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fuse the sensor data, it uses a **Kalman Filter**. Actually, it provides 2 different types of filters:\n",
    "\n",
    "* Extended Kalman Filter (EKF)\n",
    "\n",
    "\n",
    "* Unscented Kalman Filter (UKF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the practice, you can use any of them, since they will provide similar results at the end. It's up to you! Let's have a quick a look at the below image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/kalman_filter.jpg\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <span style=\"color:blue;\">BLUE</span> you can see the real state of the robot. This is, the real state of the robot.\n",
    "\n",
    "In <span style=\"color:red;\">RED</span> you can see the state of the robot based on its sensor readings, which in this case is a little bit noisy.\n",
    "\n",
    "In <span style=\"color:green;\">GREEN</span> you can see the state of the robot we get after we have applied the Kalman filter, which is much more similar to the real one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Add noise to our sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is not a required step. We are doing this step just to simulate a case where we don't have a much reliable sensor data, and to see how we can improve the readings by using the **robot_localization** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">Exercise 1.1</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firt of all, let's have a look to the current Odometry data we are receiving. For this, we are going to use RViz. So let's launch it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun rviz rviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, hit the icon with a screen in the top-right corner of the IDE window \n",
    "<img src=\"img/font-awesome_desktop.png\"/> \n",
    "in order to open the Graphic Interface window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, RViz' main screen will appear in the new window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/localization_rviz1.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's set the fixed frame to **odom**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/localization_rviz2.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add a display for visualizing the Odometry data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/add_odom_rviz.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to add a **RobotModel** display for visualizing your robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just set the topic you want to read the odometry data from. In this case it's **/odom**. You should get something similar to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/odom_rviz.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move a little bit the robot, to see how reliable our odometry data is. You can use the following command for moving the robot in circles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n",
    "  x: 0.5\n",
    "  y: 0.0\n",
    "  z: 0.0\n",
    "angular:\n",
    "  x: 0.0\n",
    "  y: 0.0\n",
    "  z: 0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the RViz screen and check the odometry data. You should see something similar to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/odom_good.gif\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the odometry data is quite reliable in this case. So... let's tweak it! For that, we are going to use the following Python script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">noisy_odom.py</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import rospy\n",
    "from nav_msgs.msg import Odometry\n",
    "import random\n",
    "\n",
    "class NoisyOdom():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.odom_subscriber = rospy.Subscriber('/odom', Odometry, self.odom_callback)\n",
    "        self.odom_publisher = rospy.Publisher('/noisy_odom', Odometry, queue_size=1)\n",
    "        self.odom_msg = Odometry()\n",
    "        self.ctrl_c = False\n",
    "        rospy.on_shutdown(self.shutdownhook)\n",
    "        self.rate = rospy.Rate(5)\n",
    "    \n",
    "    def shutdownhook(self):\n",
    "        # works better than the rospy.is_shut_down()\n",
    "        self.ctrl_c = True\n",
    "\n",
    "    def odom_callback(self, msg):\n",
    "        \n",
    "        # save the odometry message and call add_noise() function\n",
    "        self.odom_msg = msg\n",
    "        self.add_noise()\n",
    "    \n",
    "    def add_noise(self):\n",
    "        \n",
    "        # add noise to the Y position value of the odometry message\n",
    "        rand_float = random.uniform(-0.5,0.5)\n",
    "        self.odom_msg.pose.pose.position.y = self.odom_msg.pose.pose.position.y + rand_float\n",
    "        \n",
    "    def publish_noisy_odom(self):\n",
    "        \n",
    "        # loop to publish the noisy odometry values\n",
    "        while not rospy.is_shutdown():\n",
    "            self.odom_publisher.publish(self.odom_msg)\n",
    "            self.rate.sleep()\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    rospy.init_node('noisy_odom_node', anonymous=True)\n",
    "    noisyodom_object = NoisyOdom()\n",
    "    try:\n",
    "        noisyodom_object.publish_noisy_odom()\n",
    "    except rospy.ROSInterruptException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">noisy_odom.py</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the above code will add some random noise to the data published into the **/odom** topic. Specifically, to the Y component of the position. Then, it will publish this new data into a new topic named **/noisy_odom**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, create a new ROS package named **noisy_odom**, and add the above script to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, launch the code and check how the odometry looks now in RViz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">**NOTE:** When visualizing this noisy odometry, you should deactivate the covariance, since it will grow very fast.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/odom_cov_rviz.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/noisy_odom.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also move the robot again to see how the the odometry data is now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/odom_bad.gif\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the odometry is not much reliable now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now that we have broken our Odometry data... let's fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">END Exercise 1.1</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Launch the robot_localization package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have messed up our Odometry data... it's time to fix it! For that, as you already know, we are going to use the **robot_localization** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new ROS package that we are going to call **turtlebot_localization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd; cd src;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catkin_create_pkg turtlebot_localization rospy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add to our package the launch file that will start our ROS node. You can have a look at the launch below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_ekf_localization.launch**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<launch> \n",
    "\n",
    "    <!-- Run the EKF Localization node -->\n",
    "    <node pkg=\"robot_localization\" type=\"ekf_localization_node\" name=\"ekf_localization\">\n",
    "        <rosparam command=\"load\" file=\"$(find turtlebot_localization)/config/ekf_localization.yaml\"/>\n",
    "    </node>\n",
    "\n",
    "</launch>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**start_ekf_localization.launch**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it's quite simple. Basically, what we are doing here is to launch the ROS program named **ekf_localization_node** (which uses the Extended Kalman Filter), from the **robot_localization** package. Then, we are loading a series of parameters, which are stored in the configuration file named **ekf_localization.yaml**. We will fill this file later on the chapter. But before that, let's have a look at a couple of concepts that you need to know:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Understanding the reference frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot_localization node requires 4 different frames in order to work properly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **base_link_frame**: It is the frame that is in the robot itself, to which any sensor can be referenced. It is usually located in the center of the robot. It travels with it.\n",
    "\n",
    "\n",
    "* **odom_frame**: It is the frame that is used to report the odometry.\n",
    "\n",
    "\n",
    "* **map_frame**: It is the frame that is used to report a global position from a system that knows where the robot is. For instance an AMCL system. If you are not using any external Localization system, the this can be ignored.\n",
    "\n",
    "\n",
    "* **world_frame**: It is the frame that references which one of the two previous frames is going to be used to get the absolute coordinates of the robot in the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a little bit confusing though, so let's do a quick example in order to better understand this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's launch RVIz again. Next, we will add a TF display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/add_tf_rviz.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remove all the Frames and leave only the **base_link** and **odom** frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/frames_rviz3.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I would recommend to set the **alpha** variable in the **RobotModel** to something like 0.5, so that you can better visualize the frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/alpha_rviz.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, you should get something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/frames_rviz1.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the image, both frames start at the same point, which is the center of the robot. If you move a little bit the robot, though, you will see how the **odom** frame starts to separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/frames_rviz2.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for instance, and assuming that you don't have any external Localization system, the frames configuration for the robot would be like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_link_frame: base_link\n",
    "odom_frame: odom\n",
    "world_frame: odom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Adding the sensors to fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, it's time to indicate to the robot_localization node all the sensors we want to merge. The package accepts the following types of messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"http://docs.ros.org/melodic/api/nav_msgs/html/msg/Odometry.html\" target=\"_blank\">nav_msgs/Odometry</a>\n",
    "\n",
    "* <a href=\"http://docs.ros.org/melodic/api/nav_msgs/html/msg/Odometry.html\" target=\"_blank\">sensor_msgs/Imu</a>\n",
    "\n",
    "* <a href=\"http://docs.ros.org/lunar/api/geometry_msgs/html/msg/PoseWithCovarianceStamped.html\" target=\"_blank\">geometry_msgs/PoseWithCovarianceStamped</a>\n",
    "\n",
    "* <a href=\"http://docs.ros.org/melodic/api/geometry_msgs/html/msg/TwistWithCovarianceStamped.html\" target=\"_blank\">geometry_msgs/TwistWithCovarianceStamped</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any sensor that produces messages in any of these formats, can be fed to the robot_localization package to estimate the robot position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, most importantly, you can have many of each sensor. This means that, for instance, you can have the odometry provided by two different sensors (wheel encoders and visual odometry), or two different Inertial Measurement Units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look at the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/robot_localization_graph1.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, you can feed to the robot_localization node any kind of sensor, as long as it uses the type of messages specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the sensor you input must be indicated in the parameters file as the type followed by a sequential number series, starting from zero. Furthermore, you must indicate, for each sensor, the topic it's going to take the data from. The format is shown in the below example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odom0: /odom\n",
    "odom1: /visual_odom\n",
    "odom2: /laser_odom\n",
    "    \n",
    "imu0: /front_imu\n",
    "imu1: /back_imu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the above example, we are using 3 different odometry sources, and 2 different Imu sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from each of the input sensors, we must specify what variables of their messages are going to be merged (fused) in the Kalman Filter to compute the final state estimation. To specify this, you must fill a 3x5 value matrix. The matrix means the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[  X,        Y,       Z\n",
    "  roll,    pitch,    yaw\n",
    "  X/dt,     Y/dt,    Z/dt\n",
    " roll/dt, pitch/dt, yaw/dt\n",
    "  X/dt2,    Y/dt2,   Z/dt2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above values mean the following:\n",
    "\n",
    "* **X, Y, Z**: These are the [x,y,z] coordinates of the robot.\n",
    "* **roll, pitch, yaw**: These are the rpy axis, which specify the orientation of the robot.\n",
    "* **X/dt, Y/dt, Z/dt**: These are the velocities of the robot.\n",
    "* **roll/dt, pitch/dt, yaw/dt**: These are the angular velocities of the robot\n",
    "* **X/dt2, Y/dt2, Z/dt2**: These are the linear accelerations of the robot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must specify one of these matrices for each sensor. The matrix has values of **true** or **false**, indicating if we want that specific value to be taken into account by the Kalmant filter or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odom0_config: [false, false, false,\n",
    "               false, false, false,\n",
    "               true,  false, false,\n",
    "               false, false, true,\n",
    "               false, false, false]\n",
    "    \n",
    "imu0_config: [false, false, false,\n",
    "              false, false, true,\n",
    "              false, false, false,\n",
    "              false, false, true,\n",
    "              true, false, false]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this case, the values we are taking into account for the Kalman Filter are, for the odometry data:\n",
    "\n",
    "* linear velocities in X and Y, and angular velocity in Z. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the IMU data:\n",
    "\n",
    "* yaw(orientation), angular velocity in Z and linear acceleration in X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you may be asking yourself... and why this values? Why we are not using, for instance, the pose data? And that's a great question! Let's try to explain it, as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's have a look at an Odometry message, and which data it contains. For that you can execute the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header:\n",
    "  seq: 2560\n",
    "  stamp:\n",
    "    secs: 128\n",
    "    nsecs:  51000000\n",
    "  frame_id: \"odom\"\n",
    "child_frame_id: \"base_footprint\"\n",
    "pose:\n",
    "  pose:\n",
    "    position:\n",
    "      x: 0.00114332573697\n",
    "      y: 3.54915309929e-05\n",
    "      z: -0.000247248017886\n",
    "    orientation:\n",
    "      x: 0.000385212901903\n",
    "      y: -0.00720416134491\n",
    "      z: 5.46923776386e-05\n",
    "      w: 0.999973974001\n",
    "  covariance: [1e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1e-05, 0.0,0.0, 0.0, 0.0, 0.0, 0.0, 1000000000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1000000000000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001]\n",
    "twist:\n",
    "  twist:\n",
    "    linear:\n",
    "      x: -5.97116118048e-05\n",
    "      y: 0.000328280635543\n",
    "      z: 0.0\n",
    "    angular:\n",
    "      x: 0.0\n",
    "      y: 0.0\n",
    "      z: -0.000377385608995\n",
    "  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the message, the Odometry provides data related to poses:\n",
    "\n",
    "* Position in X, Y, Z.\n",
    "* Orientation in x, y, z, w. As you can see, this orientation is not expressed in the classic roll, pitch, yaw units but, instead, it's expressed in <a href=\"https://en.wikipedia.org/wiki/Quaternion\" target=\"_blank\">quaternions</a>. You can also check how to convert quaternions to rpy here: https://answers.ros.org/question/11545/plotprint-rpy-from-quaternion/#17106. \n",
    "\n",
    "And it also provides data about the velocities:\n",
    "\n",
    "* Linear velocity in X, Y, Z\n",
    "* Angular velocity in X, Y, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you you can see, with these data we are covering almost all the matrix variables, except the acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[  X,        Y,       Z\n",
    "  roll,    pitch,    yaw\n",
    "  X/dt,     Y/dt,    Z/dt\n",
    " roll/dt, pitch/dt, yaw/dt\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing we can due for sure is to set all the accelerations to **false**, since we are not getting any data about them from odometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[  ?,     ?,    ?\n",
    "   ?,     ?,    ? \n",
    "   ?,     ?,    ? \n",
    "   ?,     ?,    ?\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's have a look at rpy axis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rpy.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite obvious that our robot can only rotate on the Yaw axis. So let's set the Roll and Pitch angles to **false**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[  ?,     ?,    ?\n",
    " false, false,  ? \n",
    "   ?,     ?,    ? \n",
    "   ?,     ?,    ?\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that our robot uses a <a href=\"https://en.wikipedia.org/wiki/Differential_wheeled_robot\" target=\"_blank\">differential drive</a> to move, and that it moves in a 2D environment (It cannot move up like a drone). This means that the robot can only move linearly in the X axis, or rotationaly in the angular Z axis. Therefore, the only velocities that really matter here are the linear velocity in X and the angular velocity in Z.\n",
    "\n",
    "This will leave us with the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[  ?,     ?,    ?\n",
    " false, false,  ? \n",
    "   ?,   false, false \n",
    " false, false,    ?\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyze one last thing. In most of the cases (including this one), the odometry data is generated using a wheel encoder. This means that its velocity, orientation and position data are all generated from the same source. So, in this case, we don’t want to use all the values sinc we would be feeding duplicate information into the filter. Instead, it’s best to just use the velocities.\n",
    "\n",
    "Great! So this would leave us with the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[false, false, false\n",
    " false, false, false\n",
    "   ?,   false, false \n",
    " false, false,    ?\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, setting the left values to **true** we would have the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[false, false, false\n",
    " false, false, false\n",
    " true,  false, false \n",
    " false, false, true\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! But wait... there's one more important thing to comment on. If the odometry message reports a 0 value for linear velocity in Y (and its covariance is NOT inflated to a large value), it’s best to feed that value to the filter. As a 0 measurement in this case indicates that the robot cannot ever move in that direction, it serves as a perfectly valid measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the twist data of our odometry message, this is exactly our case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twist:\n",
    "    linear:\n",
    "      x: -5.97116118048e-05\n",
    "      y: 0.000328280635543\n",
    "      z: 0.0\n",
    "    angular:\n",
    "      x: 0.0\n",
    "      y: 0.0\n",
    "      z: -0.000377385608995\n",
    "  covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, even though it will report a 0 value always, we will also report this value to the filter. So, at the end, we will have a matrix like the below one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[false, false, false\n",
    " false, false, false\n",
    " true,  true,  false \n",
    " false, false, true\n",
    " false, false, false\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking now, though, why we didn't do the same for the linear velocity in Z. Right? Well, that's because we will actually do that using another variable, called **two_d_mode**, which you will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">Exercise 1.2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the same process in order to fill the matrix associated to the IMU data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this simulation, the IMU data is being published in the following topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/imu/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">End Exercise 1.2</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3: Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sensor is not publishing the data in the *base_link* frame of the *world_frame*, then you must take care of providing a valid ***tf* transform** between the sensor frame and the *base_link* frame.\n",
    "\n",
    "In our example, we need that transform for the Imu sensor. In this case, and usually in all simulations, this *transform* is provided by the *robot_state_publisher*. To check that the transform is actually there, you can execute the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic echo -n1 /imu/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us an output like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header:\n",
    "  seq: 0\n",
    "  stamp:\n",
    "    secs: 16\n",
    "    nsecs: 434000000\n",
    "  frame_id: \"base_footprint\"\n",
    "orientation:\n",
    "  x: 0.000384902845417\n",
    "  y: -0.00720917811173\n",
    "  z: 5.93260650974e-07\n",
    "  w: 0.999973939461\n",
    "orientation_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "angular_velocity:\n",
    "  x: -0.000515799661041\n",
    "  y: -0.00184439123128\n",
    "  z: 7.34538395099e-06\n",
    "angular_velocity_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "linear_acceleration:\n",
    "  x: 1.17275409288e-05\n",
    "  y: -1.72326132328e-06\n",
    "  z: -3.71401775708e-05\n",
    "linear_acceleration_covariance: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can see that the reference frame of our Imu sensor is **base_footprint**. Now, we can check the Frames Tree of our robot using the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd; cd src;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun tf view_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generateinto our workspace a PDF file containing the Frames Tree of our robot. You can download it and visualize it. You will have something like these:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/frames_tree.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we do have a transform from the *base_footprint* frame to the *base_link* frame, which is provided by the *robot_state_publisher*. So just bear in mind that, in case the transform is not provided for your sensor, you will need to provided it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now the have already explained the main parts you need to know for setting up the **robot_localization** node, let's actually complete our configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">Exercise 1.3</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have properly explained all the details you need to configure for the robot_localization node, let's actually create our configuration file. So, first of all, create a new folder named **config** inside your **turtlebot_localization** node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roscd robot_localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inside this folder, create the configuration file, named **ekf_localization.yaml**. Inside this file, you will place the following configuration. Complete all the parameters that are with an interrogation sign (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**ekf_localization.yaml**</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Configuation for robot odometry EKF\n",
    "#\n",
    "frequency: 50\n",
    "    \n",
    "two_d_mode: true\n",
    "    \n",
    "publish_tf: false\n",
    "\n",
    "# Complete the frames section \n",
    "odom_frame: ?\n",
    "base_link_frame: ?\n",
    "world_frame: ?\n",
    "map_frame: ?\n",
    "\n",
    "# Complete the odom0 configuration\n",
    "odom0: ?\n",
    "odom0_config: [?, ?, ?,\n",
    "               ?, ?, ?,\n",
    "               ?, ?, ?,\n",
    "               ?, ?, ?,\n",
    "               ?, ?, ?,]\n",
    "odom0_differential: false\n",
    "\n",
    "# Complete the imu0 configuration\n",
    "imu0: ?\n",
    "imu0_config: [?, ?, ?,\n",
    "              ?, ?, ?,\n",
    "              ?, ?, ?,\n",
    "              ?, ?, ?,\n",
    "              ?, ?, ?,]\n",
    "imu0_differential: false\n",
    "\n",
    "process_noise_covariance\": [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0.06, 0,     0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0.025, 0,     0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0.025, 0,    0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0.02, 0,    0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0.01, 0,    0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0.01, 0,\n",
    "                                              0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0.015]\n",
    "\n",
    "\n",
    "initial_estimate_covariance: [1e-9, 0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    1e-9, 0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    1e-9, 0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    1e-9, 0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    1e-9, 0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    1e-9, 0,    0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    1e-9, 0,    0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    1e-9, 0,    0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    1e-9, 0,     0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    1e-9,  0,     0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     1e-9,  0,     0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     1e-9,  0,    0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     1e-9, 0,    0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    1e-9, 0,\n",
    "                                                      0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    1e-9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:green;color:white;\">**ekf_localization.yaml**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain some of the parameters related to the configuration file above, which we haven't seen yet:\n",
    "\n",
    "* **frequency**: This value indicates the frequency, in Hz, at which the filter will produce estate estimations.\n",
    "\n",
    "\n",
    "* **two_d_mode**: This variable indicates if your robot is working in a 2D environment. If set to *true*, all the 3D pose variables will be set to 0.\n",
    "\n",
    "\n",
    "* **publish_tf**: This variable, if set to true, will publish the transform from the *world_frame* to the frame specified by the *base_link_frame*. In our case, this transform is already published by the *robot_state_publisher*, so we set it to *false*.\n",
    "\n",
    "\n",
    "* **sensor_differential**: This variable indicates if we want to use velocities instead of poses. So it only applies to the values in the matrix related to poses [X,Y,Z,roll,pitch,yaw]. If we want to use the pose values, then we will set it to false.\n",
    "\n",
    "\n",
    "* **process_noise_covariance**: This parameter is used to model uncertainty in the prediction stage of the filtering algorithms. WHAT!!?? Basically, it's used to improve the results produced by the filter. The values on the diagonals are the variances for the state vector, which include pose, then velocities, then linear acceleration. It is not mandatory to set, but you will achieve superior results by tunning it. Anyways, unless you are an expert on the matter, it's not easy to set at all. So, the best option in that case would be to test different values, and see how they improve or decrease the results.\n",
    "\n",
    "\n",
    "* **initial_state_covariance**: The estimate covariance defines the error in the current state estimate. This parameter allows to set the initial value for the matrix, which will affect how quickly the filter converges.\n",
    "Here's the rule you should follow: if you are measuring a variable, make the diagonal value in initial_estimate_covariance larger than that measurement's covariance. So, for example, if your measurement's covariace value for the variable in question is 1e-6, make the initial_estimate_covariance diagonal value 1e-3 or something like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want learn more about this parameters, you can check the official documentation here: http://docs.ros.org/kinetic/api/robot_localization/html/state_estimation_nodes.html#parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So we are now ready for testing how good our Kalman Filter is performing. First of all, let's make sure that we have the **noisy_odom** node running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rosrun noisy_odom noisy_odom.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's execute the EKF localization node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roslaunch robot_localization start_ekf_localization.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go again to RViz, in order to visualize what it's going on. First of all, we will visualize the noisy odometry that we have generated, right as you did in the beginning of the Chapter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/noisy_odom.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the filtered odometry. If you do *rostopic list* on your Shell, you will see that a new odometry topic has appeared, called **odometry/filtered**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:left;background: #407EAF\">\n",
    "<tr>\n",
    "<th>\n",
    "<p class=\"transparent\">Execute in WebShell #1</p>\n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rostopic list | grep odom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/noisy_odom\n",
    "/odom\n",
    "/odometry/filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new **odometry/filtered** is the filtered odometry that our **robot_localization** node is producing. So let's visualize it in Rviz! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, add another Odometry element to RViz, and do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, reduce the distance of the arrows produced by the **/noisy_odom** topic. You can do that on *Shape -> Shaft Length*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/noisy_odom_config.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, on the new Odometry display, change the color of the Arrow to blue, so that we can differentiate it from the noisy odometry. You should get something like this at the end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/good_odom.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the **robot_localization** node is filtering all the noise of the Odometry topic, and it's producing a much better odometry data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move the robot around in order to see how it performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/odom_filtered.gif\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, while the noisy odometry (**/noisy_odom**) is showing very erratic data, the filtered odometry (**/odometry/filtered**) is much more precise and stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background:#EE9023;color:white;\">END Exercise 1.3</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
